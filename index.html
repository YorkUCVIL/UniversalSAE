<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment">
  <meta name="keywords" content="Interpretability, Universal, Sparse Autoencoders, SAE, Concept">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="Universal Sparse Autoencoders">
  <meta property="og:image" content="img/fig1cap.png">
  <meta property="og:description" content="Interpretable cross-model concept alignment using sparse autoencoders.">
  <meta name="twitter:card" content="summary_large_image">
  <title>Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    p {
      margin-bottom: 10px;
    }

    .main-container {
      display: flex;
      justify-content: center;
      max-width: 1000px;
      margin: 0px auto;
    }

    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 15%;
    }

    /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
    .active,
    .collapsible:hover {
      background-color: rgb(126, 177, 219);
    }

    .main-figure-wrapper {
      width: 100%;
      margin: auto;
      text-align: center;
      position: relative;
      overflow: visible;
      /* optional, ensures overflow isn't hidden */
    }

    .main-figure-img {
      width: 115%;
      max-width: none;
      /* allow full 115% width */
      height: auto;
      position: relative;
      left: 50%;
      transform: translateX(-50%);
    }
  </style>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Universal Sparse Autoencoders: Interpretable Cross-Model Concept
              Alignment</h1>
            <div class="is-size-3 publication-authors">
              ICML 2025
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://harry-thasarathan.github.io">Harrish Thasarathan</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://j-forsyth.github.io">Julian Forsyth</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://thomasfel.fr/">Thomas Fel</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://mkowal2.github.io/">Matthew Kowal</a><sup>1,2,4,5</sup>,
              </span>
              <span class="author-block">
                <a href="https://csprofkgd.github.io/">Konstantinos G. Derpanis</a><sup>1,6,2,7</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> York University, Toronto, Canada</span>
              <span class="author-block"><sup>2</sup> Vector Institute, Toronto, Canada</span>
              <span class="author-block"><sup>3</sup> Kempner Institute, Harvard University, Boston, USA</span>
              <span class="author-block"><sup>4</sup> FAR.AI</span>
              <span class="author-block"><sup>5</sup> Trajectory Labs, Toronto</span>
              <span class="author-block"><sup>6</sup> University of Toronto, Toronto, Canada</span>
              <span class="author-block"><sup>7</sup> Samsung AI Centre, Toronto</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.03714" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/YorkUCVIL/UniversalSAE"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="demo.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-paw"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <h2 class="title is-3">
              <!-- AI vision models are increasingly diverse in their training,
        architecture, and intended tasks. But do they learn to see the world
        using the same fundamental building blocks? <br> -->
              Universal Sparse Autoencoders (USAEs) create a universal,
              interpretable concept space that reveals what multiple vision
              models learn in common about the visual world.
            </h2>
          </div>
        </div>
        <div class="main-figure-wrapper" style="margin-top: 35px;">
          <img src='img/fig1cap.png' class="main-figure-img">
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present <i>Universal Sparse Autoencoders</i> (USAEs), a framework for uncovering and aligning
              interpretable concepts spanning multiple pretrained deep neural networks.
              Unlike existing concept-based interpretability methods, which focus on a single model, USAEs jointly learn
              a universal concept space that can reconstruct and interpret the internal activations of multiple models
              at once.
              Our core insight is to train a single, overcomplete sparse autoencoder (SAE) that ingests activations from
              any model and decodes them to approximate the activations of any other model under consideration.
              By optimizing a shared objective, the learned dictionary captures common factors of
              variation—<i>concepts</i>—across different tasks, architectures, and datasets.
              We show that USAEs discover <i>semantically coherent</i> and <i>important</i> universal concepts across
              vision models; ranging from low-level features (e.g., colors and textures) to higher-level structures
              (e.g., parts and objects).
              Overall, USAEs provide a powerful new method for interpretable cross-model analysis and offer novel
              applications—such as coordinated activation maximization—that open avenues for deeper insights in
              multi-model AI systems.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--    / Abstract. -->




  <section class="section">
    <div class="container is-max-desktop">

      <!-- Method. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- <div class="content"> -->
          <h2 class="title is-3">Method Overview</h2>
          <p>
            Contrasting standard SAEs, which reinterpret the internal representations of a single model, USAEs extend
            this notion across M different models. The key insight of USAEs is to learn a shared sparse code,
            Z<!--∈ ℝⁿ×ᵐ-->, which allows every model to be reconstructed from the same sparse embedding.
            Each model has its own encoder and decoder pair to translate to and from this universal space; training
            aligns the encoders and decoders <i> across models </i> by requiring every decoder to reconstruct features
            from the shared sparse code produced by any model’s encoder.
          </p>
          <div
            style="display: flex; justify-content: center; align-items: flex-start; margin-bottom: 1em; max-width: 100%;">
            <div style="flex:1; max-width: 50%; text-align: center;">
              <img src="img/method1cap.png" style="width:100%; height:auto;">
            </div>
            <div style="width: 4%;"></div>
            <div style="flex:1; max-width: 50%; text-align: center;">
              <img src="img/method2cap.png" style="width:100%; height:auto;">

            </div>
          </div>
          <!-- </div> -->
        </div>
      </div>

      <!-- Results. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2>
          <h2 class="title is-4">Universal Concept Discovery</h2>
          <p>
            We discover a diverse variety of visual concepts in the universal space Z—that is, concepts which are
            shared and index-aligned across all models. These universal concepts range from low-level properties such as
            colour and texture (e.g., “yellow,” “curves”) to high-level semantic parts and object groupings (e.g.,
            “bolts,” “animal group faces”). Furthermore, because training is performed using spatial tokens, the
            resulting concepts exhibit spatial precision, activating only for the particular regions
            in the input images corresponding to the given concept.
          </p>
          <div style="text-align: center; margin-bottom: 40px">
            <img src="img/qualres-cap.png" width="100%" height="auto">
          </div>

          <h2 class="title is-4">Model-Unique Concepts</h2>
          <p>
            We find that some concepts are <i>not</i> universally shared between all models. DinoV2, for example,
            has many unique concepts which reflect aspects of 3-dimensional space—such as object geometry, spatial
            depth, and viewing orientation.
            SigLIP, on the other hand, has unique concepts which showcase its capability of jointly capturing
            textual-visual correspondences. A prime example is the “star” concept, activating on star
            shapes as well as the word “star”.
          </p>
          <div style="text-align: center; margin-bottom: 0px">
            <img src="img/dino3dcap.png" width="100%" height="auto">
          </div>
          <div style="text-align: center; margin-bottom: 40px">
            <img src="img/SigLIPcap.png" width="100%" height="auto">
          </div>

          <h2 class="title is-4">Visualizing Universal Concepts</h2>
          <p>
            We present an immediate application of USAEs, Coordinated Activation Maximization. For a given concept,
            optimizing the image inputs according to each model's encoder produces a per-model concept visualization.
            This reveals the different ways in which each model encodes the same concept. For example, some models
            We also demonstrate one immediate application of USAEs,
            Coordinated Activation Maximization, where optimizing the inputs of multiple models to activate the same
            concepts reveals how different models encode the same concept.
          </p>
          <div style="text-align: center; margin-bottom: 40px">
            <img src="img/CAMcap.png" width="100%" height="auto">
          </div>

          <h2 class="title is-4">The Most Universal Concepts Are Also the Most Important</h2>
          <p>
            We define <i>Concept Importance</i> by measuring the impact of a given concept on reconstruction of the
            model activations. <i>Universality</i> can be quantified by computing how often the
            neurons in the concept space of every model fire together for the same input tokens. We find a distinct
            correlation between Concept Importance and Universality. Additionally, we analyse firing entropy to
            distinguish 3 modes of concept activation: concepts which are universal across all models, concepts
            which are shared between model pairs, and concepts which are model unique.
          </p>
          <div style="text-align: center; margin-bottom: 40px">
            <img src="img/quant.png" width="80%" height="auto">
          </div>

          <h2 class="title is-4">Universal Concepts Generalise Beyond Training Data</h2>
          <p>
            While our USAE is solely trained with ImageNet, we find that its concepts generalise well to other
            datasets. This suggests that robust representational capacity can be achieved even with relatively limited
            training data.
          </p>
          <div style="text-align: center;">
            <img src="img/celeb.png" width="100%" height="auto">
          </div>


          <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
              <h2 class="title">BibTeX</h2>
              <pre><code>
@inproceedings{
thasarathan2025universal,
title={Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment},
author={Harrish Thasarathan and Julian Forsyth and Thomas Fel and Matthew Kowal and Konstantinos G. Derpanis},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=UoaxRN88oR}
}
            </code>
            </pre>
            </div>
          </section>

          <footer class="footer">
            <div class="container">
              <div class="content has-text-centered">
                <a class="icon-link" href="https://arxiv.org/pdf/2404.02233.pdf">
                  <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://yorkucvil.github.io/VCC/" class="external-link" disabled>
                  <i class="fab fa-github"></i>
                </a>
              </div>
              <div class="columns is-centered">
                <div class="column is-8">
                  <div class="content">
                    <p>
                      This website is licensed under a <a rel="license"
                        href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                      Project page based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies
                        webpage</a>.
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </footer>
</body>

</html>